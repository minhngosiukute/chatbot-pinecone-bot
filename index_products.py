# index_products.py  (Azure OpenAI + pinecone-client 3.x, CSV -> Pinecone)
import os, csv, time, sys, io
from typing import List, Dict, Any
from dotenv import load_dotenv
from openai import AzureOpenAI
from pinecone import Pinecone, ServerlessSpec

# ====== C·∫§U H√åNH ======
INDEX_NAME   = os.getenv("PINECONE_INDEX", "products-index")
PC_REGION    = os.getenv("PINECONE_REGION", "us-east-1")
EMBED_DIM    = int(os.getenv("EMBED_DIM", "1536"))  # text-embedding-3-small=1536; 3-large=3072
BATCH_SIZE   = int(os.getenv("BATCH_SIZE", "100"))
NAMESPACE    = os.getenv("PINECONE_NAMESPACE", "products")  # c√≥ th·ªÉ ƒë·ªÉ tr·ªëng
CSV_NAME     = os.getenv("PRODUCTS_CSV", "products.csv")    # ƒë·ªïi n·∫øu kh√°c
# ======================

load_dotenv()

# Azure OpenAI
AZURE_OPENAI_KEY        = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT   = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VER    = os.getenv("AZURE_OPENAI_API_VERSION", "2024-05-01-preview")
AZURE_EMBED_DEPLOY      = os.getenv("AZURE_EMBED_DEPLOY", "embedding-deploy")

# Pinecone
PINECONE_API_KEY        = os.getenv("PINECONE_API_KEY")

if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, PINECONE_API_KEY]):
    raise RuntimeError(
        "Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng. C·∫ßn AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, "
        "AZURE_OPENAI_API_VERSION, AZURE_EMBED_DEPLOY, PINECONE_API_KEY trong .env"
    )

client = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=AZURE_OPENAI_API_VER,
)
pc = Pinecone(api_key=PINECONE_API_KEY)

def _list_index_names() -> List[str]:
    # pinecone-client 3.x c√≥ th·ªÉ tr·∫£ v·ªÅ list dict ho·∫∑c object; x·ª≠ l√Ω c·∫£ hai
    names = []
    for i in pc.list_indexes():
        if isinstance(i, dict):
            names.append(i.get("name"))
        else:
            names.append(getattr(i, "name", None))
    return [n for n in names if n]

# t·∫°o index n·∫øu ch∆∞a c√≥
existing = _list_index_names()
if INDEX_NAME not in existing:
    print(f"‚è≥ Ch∆∞a c√≥ index '{INDEX_NAME}', ƒëang t·∫°o...")
    pc.create_index(
        name=INDEX_NAME,
        dimension=EMBED_DIM,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region=PC_REGION),
    )
    # ch·ªù index s·∫µn s√†ng (ƒë∆°n gi·∫£n)
    time.sleep(15)

index = pc.Index(INDEX_NAME)

# ---------- Embeddings ----------
def embed_many(texts: List[str]) -> List[List[float]]:
    # Azure OpenAI h·ªó tr·ª£ batch input (m·∫£ng string) -> 1 call
    resp = client.embeddings.create(model=AZURE_EMBED_DEPLOY, input=texts)
    # b·∫£o to√†n th·ª© t·ª±
    mapping = {d.index: d.embedding for d in resp.data}
    return [mapping[i] for i in range(len(texts))]

# ---------- CSV ----------
def _open_csv_auto(path: str):
    # ƒë·ªçc th·ª≠ utf-8-sig, fallback utf-8
    try:
        return open(path, "r", encoding="utf-8-sig")
    except:
        return open(path, "r", encoding="utf-8")

csv_path = os.path.join(os.path.dirname(__file__), CSV_NAME)
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"Kh√¥ng th·∫•y CSV: {csv_path}")

with _open_csv_auto(csv_path) as f:
    reader = csv.DictReader(f)
    fieldnames = [c.strip() for c in (reader.fieldnames or [])]

    # ch·∫•p nh·∫≠n c·∫£ local_url ho·∫∑c url
    required_any = {
        "id", "name", "description", "price",  # lu√¥n c·∫ßn
        # url c·ªôt b·∫Øt bu·ªôc: √≠t nh·∫•t m·ªôt trong hai
    }
    required_options = [{"local_url", "url"}]

    def _check_required():
        missing = required_any - set(fieldnames)
        if missing:
            raise ValueError(f"CSV thi·∫øu c·ªôt b·∫Øt bu·ªôc: {', '.join(sorted(missing))}")
        ok_option = any(bool(set(opt) & set(fieldnames)) for opt in required_options)
        if not ok_option:
            raise ValueError("CSV thi·∫øu c·ªôt URL: c·∫ßn c√≥ 'local_url' ho·∫∑c 'url'.")

    _check_required()

    rows: List[Dict[str, Any]] = []
    for r in reader:
        # normalize keys & strip
        row = {k.strip(): (v.strip() if isinstance(v, str) else v) for k, v in r.items()}

        _id = str(row.get("id", "")).strip()
        if not _id:
            continue  # b·ªè d√≤ng thi·∫øu id

        # map URL
        url = (row.get("url") or row.get("local_url") or "").strip()

        # √©p ki·ªÉu nh·∫π price, category_id
        price_val = row.get("price")
        try:
            price = float(price_val) if price_val not in (None, "", "NULL") else None
        except:
            price = None

        cat_id_val = row.get("category_id")
        try:
            category_id = int(cat_id_val) if cat_id_val not in (None, "", "NULL") else None
        except:
            category_id = None

        item = {
            "id": _id,
            "name": row.get("name", ""),
            "category": row.get("category", ""),
            "description": row.get("description", ""),
            "price": price,
            "url": url,
            "image_url": row.get("image_url", ""),
            "category_id": category_id,
            "tags": row.get("tags", ""),
        }
        rows.append(item)

if not rows:
    print("‚ö†Ô∏è CSV tr·ªëng ho·∫∑c kh√¥ng c√≥ d√≤ng h·ª£p l·ªá.")
    sys.exit(0)

# ---------- Chu·∫©n b·ªã vectors ----------
def build_doc(x: Dict[str, Any]) -> str:
    parts = [
        x.get("name", ""),
        x.get("category", ""),
        x.get("tags", ""),
        x.get("description", ""),
        f"price {x['price']}" if x.get("price") is not None else "",
    ]
    return " | ".join([p for p in parts if p])

docs = [build_doc(x) for x in rows]

print(f"üß† T·∫°o embeddings cho {len(docs)} m·ª•c...")
embs = []
# c√≥ th·ªÉ chia nh·ªè theo l√¥ n·∫øu CSV qu√° l·ªõn; ·ªü ƒë√¢y g·ªçi 1 l·∫ßn (Azure ƒë√£ h·ªó tr·ª£ batch)
# n·∫øu c·∫ßn chia l√¥: t√°ch docs th√†nh chunks r·ªìi n·ªëi k·∫øt qu·∫£
try:
    embs = embed_many(docs)
except Exception as e:
    print(f"‚ùå L·ªói t·∫°o embedding: {e}")
    sys.exit(1)

vectors = []
for it, vec in zip(rows, embs):
    vectors.append({
        "id": it["id"],
        "values": vec,
        "metadata": {
            "id": it["id"],
            "name": it["name"],
            "category": it["category"],
            "description": it["description"],
            "price": it["price"],
            "url": it["url"],
            "image_url": it["image_url"],
            "category_id": it["category_id"],
            "tags": it["tags"],
        }
    })

# ---------- Upsert ----------
total = len(vectors)
print(f"üöÄ B·∫Øt ƒë·∫ßu n·∫°p {total} s·∫£n ph·∫©m v√†o Pinecone (namespace='{NAMESPACE}')...")
for i in range(0, total, BATCH_SIZE):
    batch = vectors[i:i+BATCH_SIZE]
    index.upsert(vectors=batch, namespace=NAMESPACE if NAMESPACE else None)
    print(f"‚úÖ Upsert {min(i+BATCH_SIZE, total)}/{total}")

print(f"üéâ Ho√†n t·∫•t n·∫°p {total} d√≤ng d·ªØ li·ªáu v√†o Pinecone!")
